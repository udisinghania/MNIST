{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udit/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "df1 = pd.read_csv(\"/home/udit/Documents/cognet/train.csv\")\n",
    "X_train=np.array(df1.iloc[:33600,1:]).T\n",
    "#X_train[:,2]=X_train[:,2]/1000\n",
    "#X_train = preprocessing.normalize(X_train)\n",
    "Y_train=np.array(df1.iloc[:33600,0]).T\n",
    "#Y_train = preprocessing.normalize(Y_train)\n",
    "df2 = pd.read_csv(\"/home/udit/Documents/cognet/test.csv\")\n",
    "X_test = np.array(df1.iloc[33600:,1:]).T\n",
    "X_test = X_test/255\n",
    "#X_test = preprocessing.normalize(X_test)\n",
    "Y_test=np.array(df1.iloc[33600:,0]).T\n",
    "#print(X_train.shape)\n",
    "m=Y_train.shape[0]\n",
    "#print(m)\n",
    "Y_train=(Y_train.reshape(m,1))\n",
    "Y_test=(Y_test.reshape(Y_test.shape[0],1))\n",
    "#print(Y_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(Y_test.shape)\n",
    "#print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "        label = np.array(Y_train[1])\n",
    "\n",
    "        # The rest of columns are pixels\n",
    "        pixels = np.array(X_train.T[1][:]) \n",
    "\n",
    "        # Make those columns into a array of 8-bits pixels\n",
    "        # This array will be of 1D with length 784\n",
    "        # The pixel intensity values are integers from 0 to 255\n",
    "        pixels = np.array(pixels, dtype='uint8')\n",
    "\n",
    "        # Reshape the array into 28 x 28 array (2-dimensional array)\n",
    "        pixels = pixels.reshape((28, 28))\n",
    "\n",
    "        # Plot\n",
    "        plt.title('Label is {label}'.format(label=label))\n",
    "        plt.imshow(pixels, cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise parameters \n",
    "#Sigmoid \n",
    "#Relu\n",
    "#Sigmoid_b\n",
    "#Relu_b \n",
    "#Linear_for\n",
    "#Linear_activation_for\n",
    "#Forward_prop\n",
    "#Cost\n",
    "#Linear_b\n",
    "#Linear_activation_b\n",
    "#Backpropagation \n",
    "#Update_parameters\n",
    "#NN_model\n",
    "#Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 33600)\n"
     ]
    }
   ],
   "source": [
    "#defining one_hot encoding\n",
    "Y_t=[]\n",
    "#print(Y_train.shape)\n",
    "for i in range (len(Y_train)):\n",
    "    l=[1 if Y_train[i]==j else 0 for j in range(10)]\n",
    "    Y_t.append(np.array(l))\n",
    "Y_t=np.array(Y_t)\n",
    "Y_t=Y_t.T\n",
    "print(Y_t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 8400)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 1 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "Y_te=[]\n",
    "#print(Y_train.shape)\n",
    "for i in range (len(Y_test)):\n",
    "    l=[1 if Y_test[i]==j else 0 for j in range(10)]\n",
    "    Y_te.append(np.array(l))\n",
    "Y_te = np.array(Y_te)\n",
    "Y_te = Y_te.T\n",
    "print(Y_te.shape)\n",
    "print(Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_parameters(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    l = len(layer_dims)\n",
    "    parameters={}\n",
    "    for i in range (1,l):\n",
    "        parameters['W'+str(i)] = np.random.randn(layer_dims[i],layer_dims[i-1])*0.01\n",
    "        parameters['b'+str(i)] = np.zeros((layer_dims[i],1))\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342, 784)\n",
      "(342, 1)\n",
      "(171, 342)\n",
      "(171, 1)\n",
      "(84, 171)\n",
      "(84, 1)\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [784,40,20,10]\n",
    "#print(initialise_parameters(r))\n",
    "parameters=initialise_parameters(layers_dims)\n",
    "print(parameters['W'+str(1)].shape)\n",
    "print(parameters['b'+str(1)].shape)\n",
    "print(parameters['W'+str(2)].shape)\n",
    "print(parameters['b'+str(2)].shape)\n",
    "print(parameters['W'+str(3)].shape)\n",
    "print(parameters['b'+str(3)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid \n",
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s,z;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu\n",
    "def relu(Z):\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "     \n",
    "    cache = Z \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax\n",
    "def soft(Z):\n",
    "    Z = Z\n",
    "    t = np.exp(Z)\n",
    "    A = []\n",
    "    s=(np.sum(t,axis=0,keepdims=True))\n",
    "    s=(t/s)\n",
    "    #print(t)\n",
    "    #print(s)\n",
    "                \n",
    "    cache = Z\n",
    "    #print(cache.shape)\n",
    "    return s,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_forward\n",
    "def linear_for(A,W,b):\n",
    "    z = np.dot(W,A)+b\n",
    "    \n",
    "    cache = (A,W,b)\n",
    "    return z,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_activation_forward\n",
    "def linear_activation_for(A,W,b,fun):\n",
    "    if fun == 'sigmoid':\n",
    "      \n",
    "        z,linear_cache = linear_for(A,W,b)\n",
    "     \n",
    "        A,activation_cache = sigmoid(z)\n",
    "        #print(A.shape)\n",
    "        \n",
    "    elif fun == 'relu':\n",
    "   \n",
    "        z,linear_cache = linear_for(A,W,b)\n",
    "        A,activation_cache = relu(z)    \n",
    "        #print(A.shape)\n",
    "        \n",
    "    elif fun == \"soft\":\n",
    "        z,linear_cache = linear_for(A,W,b)\n",
    "        A,activation_cache = soft(z)\n",
    "        #print(A.shape)\n",
    "        \n",
    "    cache=(linear_cache,activation_cache)\n",
    "    return A,cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward_prop\n",
    "def forward_prop(X,parameters):\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "    caches=[]\n",
    "    \n",
    "    for i in range (1,L):\n",
    "        A_prev = A\n",
    "    \n",
    "        A,cache = linear_activation_for(A_prev,parameters['W'+str(i)],parameters['b'+str(i)],fun='relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "        \n",
    "    A,cache = linear_activation_for(A,parameters['W'+str(L)],parameters['b'+str(L)],fun='soft')\n",
    "    caches.append(cache)\n",
    "    return A,caches;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 8 ... 8 6 6]\n"
     ]
    }
   ],
   "source": [
    "A,caches = forward_prop(X_train,parameters)\n",
    "print(np.argmax(A,axis=0))\n",
    "#print(caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "def compute_cost(AL,Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    #print(Y.shape,AL)\n",
    "    cost = -1/m*(np.sum(np.sum(Y*np.log(AL))))\n",
    "    #cost=cost/(2*m)\n",
    "    \n",
    "    return cost;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3032238932497093\n"
     ]
    }
   ],
   "source": [
    "#print(A.shape)\n",
    "#print(Y_train.shape)\n",
    "c=compute_cost(A,Y_t)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(dA, cache):\n",
    "    \n",
    "   \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_back(dA , cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    #print(dA.shape)\n",
    "    #print(s.shape)\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    \n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_back(da,activation_cache):   \n",
    "    Z=activation_cache\n",
    "\n",
    "    dz=np.zeros(Z.shape)\n",
    "    for k in range(Z.shape[1]):\n",
    "        t,c=soft(Z[:,k])\n",
    "        t=np.squeeze(t)\n",
    "        SM = t.reshape((t.shape[0],1))\n",
    "        jac = np.diag(t) - np.dot(SM, SM.T)\n",
    "        dz[:,k]=np.dot(jac,da[:,k])\n",
    "  \n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_back(dZ,cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = 1/m*(np.dot(dZ,A_prev.T))\n",
    "    db = 1/m*(np.sum(dZ,axis=1, keepdims=True))\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_back(dA,cache,activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = relu_back(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_back(dZ, linear_cache)\n",
    "        \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        dZ = sigmoid_back(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_back(dZ, linear_cache)\n",
    "    \n",
    "    elif activation == \"soft\":\n",
    "        \n",
    "        dZ = soft_back(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_back(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    #Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    dAL = - (np.divide(Y, AL)) \n",
    "    \n",
    "   \n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "   \n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_back(dAL, current_cache, activation = 'soft')\n",
    "\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        \n",
    "       \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_back(grads[\"dA\"+str(l+1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters['W'+str(l+1)]-learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b'+str(l+1)]-learning_rate*grads['db'+str(l+1)]\n",
    "   \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.05, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []             \n",
    "    \n",
    "    parameters = initialise_parameters(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        \n",
    "        AL, caches = forward_prop(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 2.303224\n",
      "Cost after iteration 100: 1.755411\n",
      "Cost after iteration 200: 0.228169\n",
      "Cost after iteration 300: 0.130107\n",
      "Cost after iteration 400: 0.179107\n",
      "Cost after iteration 500: 0.103062\n",
      "Cost after iteration 600: 0.291357\n",
      "Cost after iteration 700: 0.163109\n"
     ]
    }
   ],
   "source": [
    "n_parameters = L_layer_model(X_train, Y_t, layers_dims, num_iterations = 1000, print_cost = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A,caches = forward_prop(X_train, n_parameters)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.argmax(A,axis=0))\n",
    "#print(np.argmax(Y_t,axis=0))\n",
    "q=np.argmax(A,axis=0)-np.argmax(Y_t,axis=0)\n",
    "l=list(q)\n",
    "#print(l.count(0))\n",
    "print(\"Training_Accuracy:\")\n",
    "print(l.count(0)/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.argmax(A,axis=0))\n",
    "#print(np.argmax(Y_t,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A,caches = forward_prop(X_test, n_parameters)\n",
    "print(A.shape)\n",
    "print(Y_te.shape)\n",
    "q=np.argmax(A,axis=0)-np.argmax(Y_te,axis=0)\n",
    "l=list(q)\n",
    "print(l.count(0))\n",
    "print(\"Test_Accuracy:\")\n",
    "print(l.count(0)/15)\n",
    "print(np.argmax(A,axis=0))\n",
    "print(np.argmax(Y_te,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
